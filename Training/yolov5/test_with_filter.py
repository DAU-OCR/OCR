import os
import sys
import cv2
import torch
import re
from pathlib import Path
import easyocr

# YOLOv5 ë¡œì»¬ ë ˆí¬ì—ì„œ import ê²½ë¡œ ì„¸íŒ…
FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]
sys.path.append(str(ROOT))

from models.common import DetectMultiBackend
from utils.general import (non_max_suppression, scale_boxes, xyxy2xywh)
from utils.dataloaders import LoadImages
from utils.torch_utils import select_device

# ê²½ë¡œ ì„¤ì •
weights_path = 'weights/best.pt'
source_dir = 'images/korean'
save_crop_dir = 'cropped'
save_ocr_dir = 'ocr_results'
save_vis_dir = 'visualized'

# OCR í•„í„° ì¡°ê±´
MIN_HEIGHT = 20            # ë„ˆë¬´ ì‘ì€ í…ìŠ¤íŠ¸ ë°•ìŠ¤ ë¬´ì‹œ
MIN_CONFIDENCE = 0.5       # ì‹ ë¢°ë„ ë‚®ì€ ê²°ê³¼ ë¬´ì‹œ
TEXT_PATTERN = re.compile(r'[0-9ê°€-í£]{2,}')  # ìˆ«ì+í•œê¸€ ìœ„ì£¼ í…ìŠ¤íŠ¸ë§Œ í—ˆìš©

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(save_crop_dir, exist_ok=True)
os.makedirs(save_ocr_dir, exist_ok=True)
os.makedirs(save_vis_dir, exist_ok=True)

# ë””ë°”ì´ìŠ¤ ì„ íƒ
device = select_device('cpu')

# ëª¨ë¸ ë¡œë“œ
model = DetectMultiBackend(weights_path, device=device)
stride, names, pt = model.stride, model.names, model.pt
imgsz = 640

# OCR Reader
reader = easyocr.Reader(['ko'], gpu=False)

# ì´ë¯¸ì§€ ë¡œë”©
dataset = LoadImages(source_dir, img_size=imgsz, stride=stride, auto=pt)

# ì¶”ë¡  ë£¨í”„
for path, img, im0s, vid_cap, _ in dataset:
    img = torch.from_numpy(img).to(device)
    img = img.float() / 255.0
    if img.ndimension() == 3:
        img = img.unsqueeze(0)

    pred = model(img, augment=False, visualize=False)
    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45)

    for det in pred:
        p = Path(path)
        filename = p.name
        im0 = im0s.copy()

        if len(det):
            det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], im0.shape).round()

            for j, (*xyxy, conf, cls) in enumerate(det):
                x1, y1, x2, y2 = map(int, xyxy)
                cropped_img = im0[y1:y2, x1:x2]

                # Crop ì €ì¥
                crop_filename = f"{p.stem}_plate{j+1}.jpg"
                crop_path = os.path.join(save_crop_dir, crop_filename)
                cv2.imwrite(crop_path, cropped_img)

                # OCR ìˆ˜í–‰
                ocr_result = reader.readtext(cropped_img)

                # ì‹œê°í™”ìš© ë³µì‚¬ë³¸
                vis_img = cropped_img.copy()

                # OCR ê²°ê³¼ ì¶œë ¥/í•„í„°ë§/ì‹œê°í™”
                ocr_txt_path = os.path.join(save_ocr_dir, f"{p.stem}_plate{j+1}_ocr.txt")
                with open(ocr_txt_path, 'w', encoding='utf-8') as f:
                    print(f"\nğŸ” íŒŒì¼: {filename} | ë²ˆí˜¸íŒ {j + 1}")
                    for (bbox, text, conf_score) in ocr_result:
                        y_coords = [point[1] for point in bbox]
                        box_height = max(y_coords) - min(y_coords)

                        if box_height < MIN_HEIGHT or conf_score < MIN_CONFIDENCE:
                            continue
                        if not TEXT_PATTERN.match(text):
                            continue

                        # ê²°ê³¼ ì €ì¥
                        result_str = f"{text} (ì‹ ë¢°ë„: {conf_score:.2f})"
                        print(f"â¡ï¸ ì¸ì‹ ê²°ê³¼: {result_str}")
                        f.write(result_str + '\n')

                        # ì‹œê°í™” (bbox + í…ìŠ¤íŠ¸)
                        pts = [tuple(map(int, point)) for point in bbox]
                        for i in range(4):
                            cv2.line(vis_img, pts[i], pts[(i+1)%4], (0,255,0), 2)
                        cv2.putText(vis_img, text, pts[0], cv2.FONT_HERSHEY_SIMPLEX,
                                    0.8, (0, 0, 255), 2, cv2.LINE_AA)

                # ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥
                vis_path = os.path.join(save_vis_dir, f"{p.stem}_plate{j+1}_vis.jpg")
                cv2.imwrite(vis_path, vis_img)
