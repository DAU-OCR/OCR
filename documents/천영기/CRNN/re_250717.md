# train_v1.5.1 (가상 데이터)와 v1.4.2 (실제 데이터) 학습 전략 비교 분석

- **날짜**: 2025년 7월 17일
- **분석 대상**: `ocr/train_v1_5_1.py`와 `ocr/train_v1_4_2.py`의 학습 파라미터 차이점 분석

---

## 1. 학습률 스케줄러(Learning Rate Scheduler) 변경

| 구분 | `v1.4.2` (실제 데이터) | `v1.5.1` (가상 데이터) | 변경 이유 |
| :--- | :--- | :--- | :--- |
| **스케줄러** | `OneCycleLR` | `ReduceLROnPlateau` | 데이터 특성에 맞는 안정적인 학습 전략 채택 |
| **동작 방식** | **능동적/계획적**: 정해진 계획에 따라 학습률을 **증가시켰다가 감소**시키는 사이클을 1회 실행. 복잡한 실제 데이터에서 빠르게 최적점을 찾도록 유도. | **수동적/반응적**: 검증 손실(`val_loss`)이 **개선되지 않을 때만 학습률을 감소**시킴. 깨끗하고 균일한 가상 데이터에서 과적합을 피하고 안정적으로 수렴시키기 위함. |

- **결론**: `OneCycleLR`은 복잡한 데이터에 대한 공격적인 탐색에, `ReduceLROnPlateau`는 정제된 데이터에 대한 안정적인 학습에 더 적합하다. 가상 데이터셋의 특성을 고려할 때 `ReduceLROnPlateau`는 합리적인 선택이다.

---

## 2. 주요 하이퍼파라미터(Hyperparameter) 변경

| 파라미터 | `v1.4.2` (실제 데이터) | `v1.5.1` (가상 데이터) | 변경 이유 |
| :--- | :--- | :--- | :--- |
| **`batch_size`** | 32 | 128 | **메모리 효율성 및 학습 안정성**: 가상 데이터는 이미지 크기가 균일하고 단순하여 메모리 사용량이 적다. 배치 크기를 늘려 학습 속도를 높이고, 더 안정적인 Gradient 계산이 가능하다. |
| **`epochs`** | 100 | 50 | **학습 효율성**: 가상 데이터는 양이 많고 패턴이 단순하여, 적은 반복으로도 모델이 빠르게 수렴하고 충분한 학습이 가능하다. 전체 학습 시간을 단축한다. |
| **`hidden_size`** | 512 | 256 | **과적합(Overfitting) 방지**: 가상 데이터는 패턴이 단순하여 큰 모델 용량(Capacity)이 필요 없다. 모델 크기를 줄여 불필요한 세부 특징까지 외우는 과적합을 방지하고, 일반화 성능을 높인다. |

- **결론**: 모든 하이퍼파라미터 변경은 **"단순하고, 양 많고, 균일한"** 가상 데이터셋의 특성에 맞춰 학습을 **더 빠르고, 안정적이며, 효율적으로** 진행하기 위한 최적화 과정으로 분석된다.

---

## 3. `train_v1.5.1.py` 학습 문제 진단 및 해결 시도

### 3.1. 문제점
- `train_v1_5_1.py` 스크립트를 이용한 학습에서 `Train Loss`와 `Val Loss`가 전혀 줄어들지 않고, `Val Acc`가 0.43%로 매우 낮게 유지되는 **모델 학습 실패 (Underfitting)** 현상 발생.
- 샘플 예측 결과, Ground Truth와 Prediction이 거의 일치하지 않음.

### 3.2. 초기 진단 및 확인
- **데이터 로딩 확인**: `ocr/data/virtual_data/images` 및 `labels` 폴더에 115,000개의 파일이 존재함을 확인.
- **디버깅 코드 추가**: `train_v1_5_1.py`에 디버깅 코드를 추가하여 첫 번째 배치의 이미지 텐서(`shape`, `min`, `max`)와 텍스트 레이블이 정상적으로 로드되는 것을 확인.
- **`LabelEncoder` 확인**: `ocr/utils/label_encoder.py`의 `LabelEncoder` 클래스 구현을 확인한 결과, `blank` 토큰 처리 및 `num_classes` 계산 로직은 올바른 것으로 판단.
- **결론**: 데이터 로딩 자체에는 문제가 없어 보이며, 모델이 데이터를 전혀 학습하지 못하는 근본적인 원인이 다른 곳에 있을 것으로 추정.

### 3.3. `VIRTUAL_CHARSET` 단순화 및 단일 배치 과적합 테스트
- **가설**: `VIRTUAL_CHARSET`의 복잡성(한글 조합, 자모, 영문, 숫자 등 256개 클래스) 또는 가상 이미지 생성 과정에서 모델이 학습하기 어려운 문제가 발생했을 가능성이 높음.
- **해결 시도 1 (문자셋 단순화)**: 이 가설을 검증하기 위해 `VIRTUAL_CHARSET`을 훨씬 단순한 형태(대문자 영어와 숫자)로 줄여서 모델이 학습을 시작하는지 확인하는 실험을 진행 중.
  - `ocr/train_v1_5_1.py` 파일의 `VIRTUAL_CHARSET`을 대문자 영어와 숫자로만 구성되도록 수정 완료.
  - `ocr/scripts/create_virtual_data.py` 파일의 `TOTAL_CHARS` 정의 부분을 단순화된 문자셋으로 수정 완료.
  - `ocr/scripts/create_virtual_data_val.py` 파일의 `TOTAL_CHARS` 정의 부분을 단순화된 문자셋으로 수정 완료.
- **해결 시도 2 (단일 배치 과적합 테스트)**: 모델 아키텍처, 손실 함수, 옵티마이저의 기본 학습 능력을 확인하기 위해 `train_v1_5_1.py`를 수정하여 각 에포크에서 단일 배치만 학습하도록 테스트.
  - **결과**: `Train Loss`가 0.1248에서 0.0003으로 크게 감소하며, 모델이 단일 배치에 대해 **손실을 줄이는 데 성공**했음을 확인.
  - **문제점**: 그럼에도 불구하고 `Val Acc`는 0.00%로 유지되었고, 모델은 모든 입력에 대해 **빈 문자열(`''`)만 예측**하는 현상 지속.
  - **새로운 가설**: 모델의 출력이 `blank` 토큰에 과도하게 편향되어 있거나, 디코딩 과정에서 문제가 발생했을 가능성 제기.

### 3.4. 모델 출력(Logits) 디버깅 및 결과
- **디버깅 코드 추가**: `ocr/train_v1_5_1.py`에 모델의 원시 출력(logits), `log_softmax` 적용 후 출력, 그리고 Greedy 디코딩 결과를 확인하는 디버깅 코드를 추가.
- **결과**: 
  - `Model raw output shape: torch.Size([31, 128, 37])` (예상대로 37 클래스).
  - `Model raw output sample` 및 `Log_softmax output sample`의 값들이 극단적으로 한쪽으로 치우치지 않고 분포되어 있음을 확인.
  - **`Greedy decoded text for first sample: 'N6O86LKO6K6O6K8K85BK67676'`**: 모델이 더 이상 빈 문자열을 예측하지 않고, **실제로 의미 있는 문자열을 예측하기 시작했음**을 확인. 예측된 문자들은 단순화된 `VIRTUAL_CHARSET`에 포함되는 문자들.
- **최종 분석**: 이전까지 모델이 전혀 학습되지 않고 빈 문자열만 예측했던 문제는 **복잡한 한글 문자셋 때문이었을 가능성이 매우 높음**으로 판단. 모델은 단순화된 문자셋으로 학습할 수 있는 기본적인 능력을 갖추었음이 확인됨.

### 3.5. 다음 단계 (사용자 실행 필요)
1.  **새로운 가상 데이터 생성 (필수)**: `ocr/train_v1_5_1.py`, `ocr/scripts/create_virtual_data.py`, `ocr/scripts/create_virtual_data_val.py` 파일 모두 `VIRTUAL_CHARSET`이 대문자 영어와 숫자로만 구성되도록 수정되었으므로, 이 변경 사항을 반영하여 **새로운 가상 데이터를 반드시 생성**해야 합니다.
    ```bash
    python ocr/scripts/create_virtual_data.py
    python ocr/scripts/create_virtual_data_val.py
    ```
2.  **학습 재개**: 새롭게 생성된 가상 데이터를 사용하여 `ocr/train_v1_5_1.py`를 다시 실행하여 전체 학습 과정을 관찰합니다.
    ```bash
    python ocr/train_v1_5_1.py
    ```
    `Train Loss`와 `Val Loss`가 꾸준히 감소하고 `Val Acc`가 유의미하게 증가하는지 주의 깊게 관찰해야 합니다.
